{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124de14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 1\n",
    "\n",
    "```\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import io\n",
    "# import requests\n",
    "\n",
    "# df_url = 'https://raw.githubusercontent.com/akmand/datasets/main/california_housing.csv'\n",
    "# url_content = requests.get(df_url, verify=False).content\n",
    "# data1 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n",
    "# ```\n",
    "\n",
    "\n",
    "# Follow the steps below and show the full code.\n",
    "# - Use data1.sample(105, random_state=56)  to take sample  \n",
    "# - Select [[longitude,latitude,housing_median_age,total_rooms,total_bedrooms,ocean_proximity] ] as feature and [median_house_value] as target                    \n",
    "# - Fill missing values  with mean of that column  \n",
    "# - Encode the categorical feature(s)       \n",
    "# - Split the data with test size =13% and random_state = 12         \n",
    "# - Create Decision tree model with max_depth = 10\n",
    "# - Calculate test and train accuracy\n",
    "\n",
    "\n",
    "#1st step: Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "df_url = 'https://raw.githubusercontent.com/akmand/datasets/main/california_housing.csv'\n",
    "url_content = requests.get(df_url, verify=False).content\n",
    "data1 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n",
    "\n",
    "sample1 = data1.sample(105, random_state=56)\n",
    "sample1.isna().sum()\n",
    "sample1['total_bedrooms'] = sample1['total_bedrooms'].fillna(sample1['total_bedrooms']).mean()\n",
    "sample1.isna().sum()\n",
    "sample1.head()\n",
    "sample1_ohe = pd.get_dummies(sample1, columns=['ocean_proximity']) # use one-hot-encoding to replaced categorical column\n",
    "X = sample1_ohe.drop('median_house_value', axis=1)\n",
    "y = sample1_ohe['median_house_value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.13, random_state=12)\n",
    "dt = DecisionTreeRegressor(max_depth=10, random_state=12)\n",
    "dt.fit(X_train, y_train)\n",
    "dt.score(X_train, y_train)\n",
    "dt.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0655a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2\n",
    "\n",
    "# **Use Data1 given Task 1**\n",
    "\n",
    "# Follow the steps below and show the full code.\n",
    "\n",
    "# - Use data1.sample(500, random_state=56) to take sample\n",
    "# - Select [[longitude,latitude,housing_median_age,total_rooms,total_bedrooms,ocean_proximity]] as feature and [median_house_value] as target\n",
    "# - Remove missing values with mean of that column\n",
    "# - Encode Categorical feaures\n",
    "# - Split the data with test size =13% and random_state = 12\n",
    "# - Create regression model\n",
    "# - Calculate test and train accuracy\n",
    "# - Which feature variable exhibiting the highest absolute influence on the modelâ€™s output\n",
    "\n",
    "\n",
    "sample2 = data1.sample(500, random_state=56)\n",
    "sample2.isna().sum()\n",
    "sample2['total_bedrooms'] = sample2['total_bedrooms'].fillna(sample2['total_bedrooms']).mean()\n",
    "sample2.isna().sum()\n",
    "sample2.head()\n",
    "sample2_ohe = pd.get_dummies(sample2, columns=['ocean_proximity'])\n",
    "X = sample2_ohe.drop('median_house_value', axis=1)\n",
    "y = sample2_ohe['median_house_value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.13, random_state=12)\n",
    "dtr2 = DecisionTreeRegressor(max_depth=10, random_state=12)\n",
    "dtr2.fit(X_train, y_train)\n",
    "print(f\"Train accuracy: {dtr2.score(X_train, y_train): .4f}\")\n",
    "print(f\"Test accuracy: {dtr2.score(X_test, y_test): .4f}\")\n",
    "dtr2.feature_importances_\n",
    "dtr2.feature_names_in_\n",
    "pd.Series(data = dtr2.feature_importances_, index=dtr2.feature_names_in_).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ac72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "df_url = 'https://raw.githubusercontent.com/aishwarya13-r/DAB200/refs/heads/main/circular_clusters_with_noise.csv'\n",
    "url_content = requests.get(df_url, verify=False).content\n",
    "data2 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n",
    "\n",
    "\n",
    "\n",
    "# - Visualize the data using a scatter plot (x, y) using code below\n",
    "\n",
    "\n",
    "# - Apply DBSCAN clustering and identify core\n",
    "# - Count how many points fall into noise vs clusters.\n",
    "\n",
    "\n",
    "data2.head()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data2['x'], data2['y'], s=10, color='gray')\n",
    "plt.title('Original Data: Circular Clusters with Noise')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "X = data2[['x','y']]\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "data2['cluster']\n",
    "plt.figure(figsize=(8, 6))\n",
    "unique_labels = np.unique(data2['cluster'])\n",
    "colors = {-1: 'red', 0: 'blue', 1: 'green', 2: 'purple'}  # Extend as needed\n",
    "\n",
    "for label in unique_labels:\n",
    "    mask = data2['cluster'] == label\n",
    "    cluster_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "    plt.scatter(data2[mask]['x'], data2[mask]['y'],\n",
    "                c=colors.get(label, 'gray'), s=10, label=cluster_name)\n",
    "\n",
    "plt.title(\"DBSCAN Clustering with Core and Border Points\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "n_clusters = len(set(data2['cluster'])) - (1 if -1 in data2['cluster'].values else 0)\n",
    "n_noise = np.sum(data2['cluster'] == -1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise: {n_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11144c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5\n",
    "# - Implement Agglomerative Clustering for data 2 given in task 4\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering().fit(X)\n",
    "ac_predictions=agg.fit_predict(X)\n",
    "ac_predictions\n",
    "agg.labels_\n",
    "\n",
    "\n",
    "# Option 1: Use Dendrogram\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = sch.linkage(X, method='ward')  # You can try other linkage methods like 'average', 'single', etc.\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sch.dendrogram(Z)\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Option 2: Use Silhouette Score\n",
    "\n",
    "# The silhouette score measures how well each sample fits within its cluster. A higher silhouette score indicates that the clusters are well-defined.\n",
    "\n",
    "# Silhouette score ranges from -1 to +1.\n",
    "\n",
    "# - A score close to +1 indicates that the samples are well clustered.\n",
    "# - A score close to 0 indicates overlapping clusters.\n",
    "# - A negative score suggests that samples might have been assigned to the wrong clusters.\n",
    "\n",
    "\n",
    "#Use silhouette score to find the best number of clusters:\n",
    "from sklearn.metrics import silhouette_score\n",
    "sil_score = silhouette_score(X, ac_predictions)\n",
    "print(f\"Silhouette Score: {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ba09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Part I\n",
    "\n",
    "# - Implement K-means clustering for data used in task 4\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # Set a random_state for reproducibility\n",
    "clusters2 = kmeans.fit(data2)  # Fit the KMeans model to the data\n",
    "\n",
    "clusters2.predict(data2)\n",
    "kmeans.cluster_centers_.shape\n",
    "\n",
    "\n",
    "# Assuming you have already fitted the model with kmeans.fit(data2)\n",
    "labels = kmeans.labels_  # Cluster labels for each data point\n",
    "centroids = kmeans.cluster_centers_  # Coordinates of the cluster centroids\n",
    "\n",
    "# Step 1: Plot the data points, color-coded by cluster labels\n",
    "plt.scatter(data2.iloc[:, 0], data2.iloc[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Step 2: Plot the cluster centers as red 'X' marks\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='X', label=\"Centroids\")\n",
    "\n",
    "# Add title, labels, and legend\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 1. DBSCAN clusters by density, not distance to a centroid:  \n",
    "#     - DBSCAN grows clusters by \"chaining\" together points that lie in high density\n",
    "# 2. No assumption of COnvex or Spherical shape: \n",
    "#     - KMeans optimizes around a single center and creates convex (ball-shaped) clusters. \n",
    "#     - DBSCAN links nearby points, regardless of overall shape, so it follws the curving geometry of each ring\n",
    "# 3. Automaticaly detects number of clusters: \n",
    "#     - DBSCAN discovers number of clusters based on density thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Part II\n",
    "\n",
    "# Based on the hierarchical clustering dendrogram generated using below code, if you decide to form 3 distinct clusters, identify and list the data points assigned to each of the resulting clusters\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "link = linkage(data[:10,:], method='ward')used\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(link,\n",
    "           orientation='top',\n",
    "           distance_sort='ascending',\n",
    "           show_leaf_counts=False,\n",
    "           truncate_mode='level',  # limit the depth of the tree\n",
    "           p=20)  # show only last 20 merges for readability\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Select only 'x' and 'y' for clustering\n",
    "X = data2[['x', 'y']].values\n",
    "\n",
    "link = linkage(X[:15,:], method='ward')  # X - data used\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(link,\n",
    "           orientation='top',\n",
    "           distance_sort='ascending',\n",
    "           show_leaf_counts=False,\n",
    "           truncate_mode='level')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
