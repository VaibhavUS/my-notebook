{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "authorship_tag": "ABX9TyM6thDoiceJLgjQjxrRNVaw",
      "provenance": []
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Q1\n\nIf we use X = np.array([[-2.69305227, 0.55617734, -2.43874732, 2.07026181], [ 1.46140931, 2.56987791, 1.40453353, 1.36132903], [-0.7035427 , 0.92223751, 0.36613864, -1.59524025], [ 1.20800429, -0.4637739 , 1.1106433 , 0.85284503], [-2.23840524, -0.84209178, -0.04810615, 1.53559851], [ 3.44829863, 2.13786731, 0.9661306 , -3.02757453], [ 4.19588081, 0.95286342, 0.16051249, 0.59197391], [ 0.1426253 , -1.25490369, 0.0986404 , 0.6355671 ], [ 0.27103178, 0.52906105, -0.64250317, -1.29833464], [-0.08165368, -0.87659034, -3.02019504, 1.0802352 ], [ 0.52249028, -1.72924316, 1.21902947, 1.38806363], [-0.23223567, -1.22051892, 2.88914811, -0.29774035], [-1.04524743, -1.27354275, -1.0832457 , 0.05064772], [ 0.51773799, -1.61353239, 0.13621013, -2.08071959]]) and y = np.array([2, 3, 0, 4, 3, 0, 3, 0, 1, 1, 4, 1, 2, 2]) to train a decision tree (with random_state = 146), what are the predictions for X_test = np.array([[ 0.59730683, -1.47388234, 0.19323579, 1.00210836], [ 1.40572379, 1.44596069, -0.72924456, -1.5125544 ], [-2.38342167, -0.36546006, 0.77207181, 2.69580243], [ 1.26963 , -1.74360923, 0.26511791, -2.56548327], [-3.7681268 , 1.71391718, -3.59390654, 3.78457425]])?",
      "metadata": {
        "id": "gHeXxPK7azET"
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = np.array([[-2.69305227, 0.55617734, -2.43874732, 2.07026181], [ 1.46140931, 2.56987791, 1.40453353, 1.36132903], [-0.7035427 , 0.92223751, 0.36613864, -1.59524025], [ 1.20800429, -0.4637739 , 1.1106433 , 0.85284503], [-2.23840524, -0.84209178, -0.04810615, 1.53559851], [ 3.44829863, 2.13786731, 0.9661306 , -3.02757453], [ 4.19588081, 0.95286342, 0.16051249, 0.59197391], [ 0.1426253 , -1.25490369, 0.0986404 , 0.6355671 ], [ 0.27103178, 0.52906105, -0.64250317, -1.29833464], [-0.08165368, -0.87659034, -3.02019504, 1.0802352 ], [ 0.52249028, -1.72924316, 1.21902947, 1.38806363], [-0.23223567, -1.22051892, 2.88914811, -0.29774035], [-1.04524743, -1.27354275, -1.0832457 , 0.05064772], [ 0.51773799, -1.61353239, 0.13621013, -2.08071959]])\n\ny = np.array([2, 3, 0, 4, 3, 0, 3, 0, 1, 1, 4, 1, 2, 2])\n\nX_test = np.array([[ 0.59730683, -1.47388234, 0.19323579, 1.00210836], [ 1.40572379, 1.44596069, -0.72924456, -1.5125544 ], [-2.38342167, -0.36546006, 0.77207181, 2.69580243], [ 1.26963 , -1.74360923, 0.26511791, -2.56548327], [-3.7681268 , 1.71391718, -3.59390654, 3.78457425]])\n\nclf = DecisionTreeClassifier(random_state=146)\n\nclf = clf.fit(X, y)\nclf.predict(X_test)",
      "metadata": {
        "id": "ptLHaxg2wSe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 0, 3, 4, 3])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": "Q2\n\nIf we use X = np.array([[-2.69305227, 0.55617734, -2.43874732, 2.07026181], [ 1.46140931, 2.56987791, 1.40453353, 1.36132903], [-0.7035427 , 0.92223751, 0.36613864, -1.59524025], [ 1.20800429, -0.4637739 , 1.1106433 , 0.85284503], [-2.23840524, -0.84209178, -0.04810615, 1.53559851], [ 3.44829863, 2.13786731, 0.9661306 , -3.02757453], [ 4.19588081, 0.95286342, 0.16051249, 0.59197391], [ 0.1426253 , -1.25490369, 0.0986404 , 0.6355671 ], [ 0.27103178, 0.52906105, -0.64250317, -1.29833464], [-0.08165368, -0.87659034, -3.02019504, 1.0802352 ], [ 0.52249028, -1.72924316, 1.21902947, 1.38806363], [-0.23223567, -1.22051892, 2.88914811, -0.29774035], [-1.04524743, -1.27354275, -1.0832457 , 0.05064772], [ 0.51773799, -1.61353239, 0.13621013, -2.08071959]]) and y = np.array([2, 3, 0, 4, 3, 0, 3, 0, 1, 1, 4, 1, 2, 2]) to train KNN with K = 5, what are the predictions for X_test = np.array([[ 0.59730683, -1.47388234, 0.19323579, 1.00210836], [ 1.40572379, 1.44596069, -0.72924456, -1.5125544 ], [-2.38342167, -0.36546006, 0.77207181, 2.69580243], [ 1.26963 , -1.74360923, 0.26511791, -2.56548327], [-3.7681268 , 1.71391718, -3.59390654, 3.78457425]])?",
      "metadata": {
        "id": "0GMkJM_7wXux"
      }
    },
    {
      "cell_type": "code",
      "source": "X = np.array([[-2.69305227, 0.55617734, -2.43874732, 2.07026181], [ 1.46140931, 2.56987791, 1.40453353, 1.36132903], [-0.7035427 , 0.92223751, 0.36613864, -1.59524025], [ 1.20800429, -0.4637739 , 1.1106433 , 0.85284503], [-2.23840524, -0.84209178, -0.04810615, 1.53559851], [ 3.44829863, 2.13786731, 0.9661306 , -3.02757453], [ 4.19588081, 0.95286342, 0.16051249, 0.59197391], [ 0.1426253 , -1.25490369, 0.0986404 , 0.6355671 ], [ 0.27103178, 0.52906105, -0.64250317, -1.29833464], [-0.08165368, -0.87659034, -3.02019504, 1.0802352 ], [ 0.52249028, -1.72924316, 1.21902947, 1.38806363], [-0.23223567, -1.22051892, 2.88914811, -0.29774035], [-1.04524743, -1.27354275, -1.0832457 , 0.05064772], [ 0.51773799, -1.61353239, 0.13621013, -2.08071959]])\n\ny = np.array([2, 3, 0, 4, 3, 0, 3, 0, 1, 1, 4, 1, 2, 2])\n\nX_test = np.array([[ 0.59730683, -1.47388234, 0.19323579, 1.00210836], [ 1.40572379, 1.44596069, -0.72924456, -1.5125544 ], [-2.38342167, -0.36546006, 0.77207181, 2.69580243], [ 1.26963 , -1.74360923, 0.26511791, -2.56548327], [-3.7681268 , 1.71391718, -3.59390654, 3.78457425]])\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf=KNeighborsClassifier(n_neighbors=5)\nclf=clf.fit(X,y)\nclf.predict(X_test)\n",
      "metadata": {
        "id": "YNZy7wm2wd-e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 0, 2, 0, 2])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport io\nimport requests\n\n# dataset1 (Data1)\ndf_url = 'https://raw.githubusercontent.com/akmand/datasets/master/openintro/bdims.csv'\nurl_content = requests.get(df_url, verify=False).content\ndata1 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n\n# dataset2 (Data2)\ndf_url = 'https://raw.githubusercontent.com/akmand/datasets/main/baseball.csv'\nurl_content = requests.get(df_url, verify=False).content\ndata2 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))  \n\n# dataset3 (Data3)\ndf_url = 'https://raw.githubusercontent.com/akmand/datasets/main/cdc.csv'\nurl_content = requests.get(df_url, verify=False).content\ndata3 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n\n\nQ3\n\n\nIf data1.sample(n=91,random_state=24) is your data where 'sex' is target, what would be the out-of-bag score for baseline random forest model with 97 trees and random_state = 24?\n\n\n0.8797989010989011\n\n0.9164989010989011\n\n0.9010989010989011\n\n0.89890109814250",
      "metadata": {
        "id": "6ZkacwL9wrv3"
      }
    },
    {
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport io\nimport requests\n\n# dataset1 (Data1)\ndf_url = 'https://raw.githubusercontent.com/akmand/datasets/master/openintro/bdims.csv'\nurl_content = requests.get(df_url, verify=False).content\ndata1 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n\n# dataset2 (Data2)\ndf_url = 'https://raw.githubusercontent.com/akmand/datasets/main/baseball.csv'\nurl_content = requests.get(df_url, verify=False).content\ndata2 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))  \n\n# dataset3 (Data3)\ndf_url = 'https://raw.githubusercontent.com/akmand/datasets/main/cdc.csv'\nurl_content = requests.get(df_url, verify=False).content\ndata3 = pd.read_csv(io.StringIO(url_content.decode('utf-8')))",
      "metadata": {},
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": "sample1 = data1.sample(n=91,random_state=24)\nsample1.isna().sum()\nX = sample1.drop(\"sex\", axis=1)\ny = sample1[['sex']]\nfrom sklearn.ensemble import *\nclf = RandomForestClassifier(n_estimators=97, random_state=24,oob_score=True)\n\nclf.fit(X,y)\nclf.oob_score_",
      "metadata": {
        "id": "vaeu-vn1w125"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9010989010989011"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": "print(\"Null values in Data1:\")\nprint(data1.isnull().sum())\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Sample the data\nsampled_data = data1.sample(n=91, random_state=24)\n\n# Separate features (X) and target (y)\nX = sampled_data.drop('sex', axis=1)  # Assuming 'sex' is the target column\ny = sampled_data['sex']\n\n# Create and train the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=97, oob_score=True, random_state=24)\nrf_model.fit(X, y)\n\n# Get the out-of-bag score\noob_score = rf_model.oob_score_\n\n# Print the result\nprint(f\"Out-of-bag score: {oob_score}\")",
      "metadata": {
        "id": "r1WSBqeow3wo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Null values in Data1:\n,bia.di    0\n,bii.di    0\n,bit.di    0\n,che.de    0\n,che.di    0\n,elb.di    0\n,wri.di    0\n,kne.di    0\n,ank.di    0\n,sho.gi    0\n,che.gi    0\n,wai.gi    0\n,nav.gi    0\n,hip.gi    0\n,thi.gi    0\n,bic.gi    0\n,for.gi    0\n,kne.gi    0\n,cal.gi    0\n,ank.gi    0\n,wri.gi    0\n,age       0\n,wgt       0\n,hgt       0\n,sex       0\n,dtype: int64\n,Out-of-bag score: 0.9010989010989011\n"
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": "Q4\n\n\nYou will be using the data1 dataset for this question. Using the features ['bia.di', 'bii.di', 'bit.di', 'che.de', 'elb.di', 'wri.di', 'kne.di', 'ank.di'], create a RandomForest model to predict a person's sex. Split the data into training (80%) and testing (20%) sets using random_state=128. Train the RandomForest model with n_estimators=50, max_depth=7, and random_state=128.\n\nPrint the score for testing data and oob score for the model.\n\n1.   List item\n2.   List item\n\n",
      "metadata": {
        "id": "d31iFJP-w_Ui"
      }
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfeatures = ['bia.di', 'bii.di', 'bit.di', 'che.de', 'elb.di', 'wri.di', 'kne.di', 'ank.di']\nX = data1[features]\ny = data1['sex']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=128)\n\nrfc = RandomForestClassifier(n_estimators=50, max_depth=7, random_state=128, oob_score=True)\n\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\n\nrfc.oob_score_\n0.928395061728395\n\naccuracy_score(y_test, y_pred)\n0.9509803921568627",
      "metadata": {
        "id": "U0SwGVaxxDdE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9509803921568627"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": "Q5\n\nIf data3.sample(n=2861,random_state=417) is your data and you apply one hot encoding on 'genhlth' feature of sample data. findout the data at index 1686 and match with following option.\n\nA) [[1, 1, 1, 63, 132, 125, 49, 'f', 0, 0, 0, 0, 1]]\n\nB) [[1, 4, 1, 63, 132, 125, 49, 'f', 0, 0, 0, 0, 1]]\n\nC) [[1, 1, 1, 63, 115, 125, 49, 'f', 0, 1, 0, 0, 1]]",
      "metadata": {
        "id": "vbkGIWGCxF91"
      }
    },
    {
      "cell_type": "code",
      "source": "data3_new = data3.sample(n=2861,random_state=417)\n\none_hot_data3 = pd.get_dummies(data3_new.genhlth)\n\ndata3_new = data3_new.join(one_hot_data3)\n\ndata3_new.iloc[1686]\n\n# A) [[1, 1, 1, 63, 132, 125, 49, 'f', 0, 0, 0, 0, 1]]",
      "metadata": {
        "id": "Qu9GhYIXxJc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genhlth      very good\n",
              "exerany              1\n",
              "hlthplan             1\n",
              "smoke100             1\n",
              "height              63\n",
              "weight             132\n",
              "wtdesire           125\n",
              "age                 49\n",
              "gender               f\n",
              "excellent        False\n",
              "fair             False\n",
              "good             False\n",
              "poor             False\n",
              "very good         True\n",
              "Name: 6614, dtype: object"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": "Q6\n\nIf data3.sample(n=2803,random_state=130) is your data and you apply providing mapping ('good':6, 'very good':11, 'excellent':22, 'fair':32, 'poor':49) on 'genhlth' feature of sample data. findout the data at index 1153 and match with following option.\n\nA) [[32, 1, 1, 0, 74, 230, 200, 66, 'm']]\nB) [[11, 0, 1, 0, 71, 250, 220, 32, 'm']]\nC) [[6, 0, 1, 1, 77, 235, 235, 27, 'm']]",
      "metadata": {
        "id": "DRUg4kiKxOhW"
      }
    },
    {
      "cell_type": "code",
      "source": "data3_3 = data3.sample(n=2803,random_state=130)\n\nmapping = {'good':6, 'very good':11, 'excellent':22, 'fair':32, 'poor':49}\n\ndata3_3.genhlth = data3_3.genhlth.map(mapping)\n\ndata3_3.iloc[1153]\n\n# B) [[11, 0, 1, 0, 71, 250, 220, 32, 'm']]",
      "metadata": {
        "id": "ofbX-sBRxTzc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genhlth      11\n",
              "exerany       0\n",
              "hlthplan      1\n",
              "smoke100      0\n",
              "height       71\n",
              "weight      250\n",
              "wtdesire    220\n",
              "age          32\n",
              "gender        m\n",
              "Name: 13641, dtype: object"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": "1)\n\nsample3=data3.sample(n=2861,random_state=417)\nonehtencd= pd.get_dummies(sample3[\"genhlth\"])\nsample3=sample3.join(onehtencd)\nsample3.iloc[1686]\n\n2)sample4=data3.sample(n=2803,random_state=130)\nmapping={'good':6, 'very good':11, 'excellent':22, 'fair':32, 'poor':49}\nsample4.genhlth=sample4.genhlth.map(mapping)##so here it is replacing numbers with values that is mapping nos with values for eg 1153 which we find it is rplacing very good with 11\nsample4.iloc[1153]\n\n3)\n# With using sklearn\n\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(preds,y)\nmse\n\n# With using sklearn\n\nfrom sklearn.metrics import mean_squared_error\nmae=mean_absolute_error(preds,y)\nmse\n\n# wothpout sklear(mse)\nmse=np.sum((y-preds)**2)/len(y)\nmse\n\n#without sklearn(rmse)\nmae=np.sum((y-preds)**2)/len(y)\nmae\nrmse=np.sqrt(mae)\nrmse",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
